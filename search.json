[
  {
    "objectID": "5-Predictive Modelling/Predictive_modelling.html#final-verdict-predictive-modelling-performance",
    "href": "5-Predictive Modelling/Predictive_modelling.html#final-verdict-predictive-modelling-performance",
    "title": "5. Predictive Modelling: Urban Weather Analysis",
    "section": "Final Verdict: Predictive Modelling Performance",
    "text": "Final Verdict: Predictive Modelling Performance\nThis section concludes our investigation into the drivers of urban thermal perception. By pitting a traditional linear baseline against a high-dimensional ensemble model, we have quantified the relationship between geography, urban scale, and the human ‚Äúfeels like‚Äù experience.\n\n1. The Statistical Comparison\nOur modeling efforts resulted in a clear hierarchical performance gap, proving that environmental data requires non-linear interpretation:\n\n\n\nModel\n\\(R^2\\) Score (Accuracy)\nMean Absolute Error (MAE)\n\n\n\n\nLinear Regression\n0.7590\n6.06¬∞C\n\n\nRandom Forest\n0.8945\n3.60¬∞C\n\n\n\nConclusion: The Random Forest model achieved a 40.6% reduction in error, effectively capturing the ‚Äújagged‚Äù reality of weather patterns that a linear plane could not.\n\n\n2. Feature Importance Breakdown: What Drives the Model?\nThe model ‚Äúinterrogated‚Äù the features and established a clear hierarchy of influence:\n\nLatitude (69.2%): The undisputed ‚ÄúKing‚Äù of predictors. Geography dictates the global thermal baseline.\nPressure (19.8%): A surprise ‚Äúsuper-feature‚Äù that proved more predictive of perceived comfort than raw humidity alone.\nLongitude (4.5%): Captured regional/maritime climate variations across continents.\nUrban & Engineered Factors (3.0%): While Population_scaled and our moisture_wind_ratio provided essential ‚Äúfine-tuning‚Äù for urban micro-climates, they were secondary to the massive influence of planetary positioning.\n\n\n\n3. Key Findings & Thesis Validation\n\nThesis Status: Validated. We have demonstrated that Apparent Temperature can be predicted with ~90% accuracy using exclusively size, location, and atmospheric data.\nThe ‚ÄúInteraction‚Äù Effect: The superior performance of the Random Forest proves that perceived temperature is an interaction, not a sum. For example, the model learned that high humidity feels significantly more oppressive at specific pressures and latitudes.\nUrban Heat Island (UHI) vs.¬†Geography: While Population_scaled was a top-tier predictor, its 2.1% importance suggests that on a global scale, where a city is located (Latitude) is significantly more impactful than how big the city is.\n\n\n\n4. Real-World Impact: Sample Comparison\nIn practical terms, the Random Forest predicted city temperatures with a precision of ¬±3.60¬∞C. In contrast, the Linear Regression‚Äôs error of 6.06¬∞C represents the difference between a mild day and a shivering one, highlighting the necessity of complex models in climate-based predictive tasks."
  },
  {
    "objectID": "2-Preliminary-EDA/Preliminary-EDA.html#understanding-the-skeleton-structure",
    "href": "2-Preliminary-EDA/Preliminary-EDA.html#understanding-the-skeleton-structure",
    "title": "2. Preliminary EDA & Visualization",
    "section": "2.1 Understanding the ‚ÄúSkeleton‚Äù (Structure)",
    "text": "2.1 Understanding the ‚ÄúSkeleton‚Äù (Structure)\nFirst, we use basic Pandas commands to see what we are working with. You should always start here to check for missing values or unexpected data types.\n\n\nCode\n# --- CITY DATA INSPECTION ---\nprint(f\"Cities Dataset Dimensions: {raw_data.shape}\")\n\n# info() shows us non-null counts and memory usage\nprint(\"\\n--- City Data Structure ---\")\nraw_data.info()\n\n# Check the coordinates‚Äîimportant for mapping later!\nprint(\"\\n--- Geographical Range ---\")\nprint(f\"Latitude Range: {raw_data['Latitude'].min()} to {raw_data['Latitude'].max()}\")\nprint(f\"Longitude Range: {raw_data['Longitude'].min()} to {raw_data['Longitude'].max()}\")\n\n\nüèôÔ∏è Cities Dataset Dimensions: (5000, 4)\n\n--- City Data Structure ---\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   City        5000 non-null   object \n 1   Population  5000 non-null   float64\n 2   Latitude    5000 non-null   float64\n 3   Longitude   5000 non-null   float64\ndtypes: float64(3), object(1)\nmemory usage: 156.4+ KB\n\n--- Geographical Range ---\nLatitude Range: -53.15 to 69.3535\nLongitude Range: -157.8583333 to 176.166667\n\n\n\n\nCode\n# --- WEATHER DATA INSPECTION ---\nprint(f\"Weather Dataset Dimensions: {live_weather_df.shape}\")\n\n# Statistics for Temperature, Humidity, and Wind\nprint(\"\\n--- Numerical Statistics (Weather) ---\")\ndisplay(live_weather_df.describe())\n\n# Categorical check: What are the most common conditions?\nprint(\"\\n--- Sky Conditions Summary ---\")\nprint(live_weather_df['condition'].value_counts())\n\n\nüå°Ô∏è Weather Dataset Dimensions: (492, 7)\n\n--- Numerical Statistics (Weather) ---\n\n\n\n    \n\n\n\n\n\n\ntemp\nfeels_like\nhumidity\npressure\nwind\n\n\n\n\ncount\n492.000000\n492.000000\n492.000000\n492.000000\n492.000000\n\n\nmean\n13.214370\n12.063618\n66.819106\n1017.063008\n3.398191\n\n\nstd\n13.896821\n15.910123\n21.708535\n9.007922\n2.187702\n\n\nmin\n-23.930000\n-30.460000\n6.000000\n994.000000\n0.000000\n\n\n25%\n3.507500\n0.137500\n53.750000\n1012.000000\n1.897500\n\n\n50%\n15.070000\n14.130000\n70.000000\n1016.000000\n3.015000\n\n\n75%\n25.302500\n25.742500\n83.000000\n1021.000000\n4.640000\n\n\nmax\n38.650000\n39.950000\n100.000000\n1042.000000\n12.520000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n--- Sky Conditions Summary ---\ncondition\novercast clouds                 139\nclear sky                       115\nbroken clouds                    64\nscattered clouds                 45\nfew clouds                       36\nhaze                             30\nlight rain                       16\nmist                             15\nfog                               6\nlight snow                        6\nsnow                              5\nsmoke                             5\nmoderate rain                     4\nthunderstorm                      2\nlight intensity shower rain       1\nsand                              1\nthunderstorm with light rain      1\nlight rain and snow               1\nName: count, dtype: int64"
  },
  {
    "objectID": "2-Preliminary-EDA/Preliminary-EDA.html#merging-our-datasets",
    "href": "2-Preliminary-EDA/Preliminary-EDA.html#merging-our-datasets",
    "title": "2. Preliminary EDA & Visualization",
    "section": "2.2 Merging Our Datasets",
    "text": "2.2 Merging Our Datasets\nTo analyze how weather relates to population, we need to combine our two separate tables into one Master DataFrame.\n\nWhy this logic?\n\nStandardizing: Computers see ‚ÄúLondon‚Äù and ‚Äúlondon‚Äù (with a space) as different places. We use .str.lower().str.strip() to make the names identical so they match perfectly.\nThe ‚ÄúInner‚Äù Merge: This keeps only the cities that appear in both lists. Since we only have weather for ~50 cities, this filters our millions of rows down to a focused dataset.\nRenaming for Speed: We rename Population to pop simply to make it faster to type during our visualization steps!\n\n\n\nCode\n# --- THE CORRECTED MERGE STEP ---\n\n# 1. Standardize the keys\n# Use .str.lower().str.strip() to ensure both operations are applied to the text\nraw_data['city_match'] = raw_data['City'].str.lower().str.strip()\nlive_weather_df['city_match'] = live_weather_df['city_name'].str.lower().str.strip()\n\n# 2. Perform the Merge\n# Using 'raw_data' (your city list) and 'live_weather_df'\nmaster_df = pd.merge(\n    live_weather_df,\n    raw_data[['city_match', 'Population', 'Latitude', 'Longitude']],\n    on='city_match',\n    how='inner'\n)\n\n# 3. Clean up\nmaster_df = master_df.drop(columns=['city_match'])\nmaster_df = master_df.rename(columns={'Population': 'pop'})\n\nprint(f\"Merge Complete! Final Dataset Dimensions: {master_df.shape}\")\nmaster_df.head()\n\n\n‚úÖ Merge Complete! Final Dataset Dimensions: (528, 10)\n\n\n\n    \n\n\n\n\n\n\ncity_name\ntemp\nfeels_like\nhumidity\npressure\ncondition\nwind\npop\nLatitude\nLongitude\n\n\n\n\n0\ntokyo\n4.31\n4.31\n47\n1014\nlight rain\n0.45\n31480498.0\n35.685000\n139.751389\n\n\n1\nshanghai\n7.92\n4.27\n66\n1030\nscattered clouds\n7.00\n14608512.0\n31.045556\n121.399722\n\n\n2\nbombay\n27.99\n28.53\n51\n1013\nsmoke\n3.60\n12692717.0\n18.975000\n72.825833\n\n\n3\nkarachi\n22.90\n21.85\n23\n1017\novercast clouds\n4.12\n11627378.0\n24.905600\n67.082200\n\n\n4\nnew delhi\n15.09\n14.67\n77\n1018\nmist\n2.06\n10928270.0\n28.600000\n77.200000"
  },
  {
    "objectID": "2-Preliminary-EDA/Preliminary-EDA.html#the-big-picture-analysis-pairplots",
    "href": "2-Preliminary-EDA/Preliminary-EDA.html#the-big-picture-analysis-pairplots",
    "title": "2. Preliminary EDA & Visualization",
    "section": "2.3 The ‚ÄúBig Picture‚Äù Analysis (Pairplots)",
    "text": "2.3 The ‚ÄúBig Picture‚Äù Analysis (Pairplots)\nOnce we know our data is clean, we want to see how different variables interact. Instead of making dozens of individual charts, we use a Pairplot. This is a powerful ‚Äúall-in-one‚Äù visualization that helps us spot patterns across the entire dataset instantly.\n\n\nWhat is happening in this visualization?\nA Pairplot creates a matrix of charts. To read it like a pro, look at these three distinct areas:\n\nThe Diagonal (The Curves): Running from the top-left to the bottom-right, you see Distribution Plots (KDE). These show you the ‚Äúshape‚Äù of a single variable‚Äîfor example, where most city temperatures are concentrated.\nThe Scatter Plots (The Dots): Every other square shows the relationship between two different variables.\n\n\nIf the dots form a clear line, the variables are correlated.\nIf the dots look like a messy cloud, there is no relationship.\n\n\nThe Red Regression Lines (kind='reg'): These lines represent the ‚Äúbest fit‚Äù for the data.\n\n\nSloping Up: As one value goes up, the other tends to go up (e.g., Humidity vs.¬†Feels Like).\nSloping Down: As one value goes up, the other tends to go down.\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. We select only the numerical columns that make sense for a beginner to compare\n# We exclude lat/lng for now to focus on the 'Weather vs People' story\ncols_to_plot = ['pop', 'temp', 'feels_like', 'humidity', 'wind']\n\n# 2. Create the Pairplot\n# 'diag_kind' shows the univariate distribution (histogram) on the diagonal\n# 'kind=reg' adds a trend line to the scatter plots to show correlations\nplot = sns.pairplot(\n    master_df[cols_to_plot],\n    diag_kind='kde',\n    kind='reg',\n    plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.4}}\n)\n\n# Add a title to the figure\nplot.fig.suptitle(\"The 'Big Picture': Multi-Variable Relationships\", y=1.02, fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Linear_Regression.html#the-setup",
    "href": "Extra ML Material/Linear_Regression.html#the-setup",
    "title": "Linear Regression: A ‚ÄúHello World‚Äù in Machine Learning",
    "section": "1. The Setup",
    "text": "1. The Setup\nWe‚Äôll use numpy for data generation, matplotlib for visualization, and scikit-learn for the model.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "Extra ML Material/Linear_Regression.html#generate-synthetic-data",
    "href": "Extra ML Material/Linear_Regression.html#generate-synthetic-data",
    "title": "Linear Regression: A ‚ÄúHello World‚Äù in Machine Learning",
    "section": "2. Generate Synthetic Data",
    "text": "2. Generate Synthetic Data\nWe‚Äôre creating a dataset where \\(y = 4 + 3x + \\text{noise}\\). * X: Our independent feature. * y: Our target variable (what we want to predict).\n\n\nCode\n# Seed for reproducibility\nnp.random.seed(42)\n\n# Generate 100 data points\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Split into Training and Testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
  },
  {
    "objectID": "Extra ML Material/Linear_Regression.html#visualize-the-real-world",
    "href": "Extra ML Material/Linear_Regression.html#visualize-the-real-world",
    "title": "Linear Regression: A ‚ÄúHello World‚Äù in Machine Learning",
    "section": "3. Visualize the ‚ÄúReal‚Äù World",
    "text": "3. Visualize the ‚ÄúReal‚Äù World\nBefore modeling, it‚Äôs always smart to see what we‚Äôre working with. We are looking for a linear trend.\n[Image of a scatter plot showing a linear relationship with noise]\n\n\nCode\nplt.scatter(X, y, color='blue', s=10)\nplt.xlabel(\"Independent Variable (X)\")\nplt.ylabel(\"Target Variable (y)\")\nplt.title(\"Synthetic Data Distribution\")\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Linear_Regression.html#train-the-model",
    "href": "Extra ML Material/Linear_Regression.html#train-the-model",
    "title": "Linear Regression: A ‚ÄúHello World‚Äù in Machine Learning",
    "section": "4. Train the Model",
    "text": "4. Train the Model\nWe initialize the LinearRegression object and ‚Äúfit‚Äù it. This is where the model calculates the optimal slope and intercept.\n\nNote: The model is trying to solve the equation \\(y = mx + b\\). The Coefficient is your slope (\\(m\\)), and the Intercept is your constant (\\(b\\)).\n\n\n\nCode\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Print the results\nprint(f\"Intercept: {model.intercept_[0]:.2f}\")\nprint(f\"Coefficient: {model.coef_[0][0]:.2f}\")\n\n\nIntercept: 4.13\nCoefficient: 2.77"
  },
  {
    "objectID": "Extra ML Material/Linear_Regression.html#make-predictions-plot-the-best-fit-line",
    "href": "Extra ML Material/Linear_Regression.html#make-predictions-plot-the-best-fit-line",
    "title": "Linear Regression: A ‚ÄúHello World‚Äù in Machine Learning",
    "section": "5. Make Predictions & Plot the Best-Fit Line",
    "text": "5. Make Predictions & Plot the Best-Fit Line\nNow we use our ‚ÄúTest‚Äù data to see how the model performs on information it hasn‚Äôt seen yet.\n\n\nCode\ny_pred = model.predict(X_test)\n\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.plot(X_test, y_pred, color='red', linewidth=2, label='Best-Fit Line')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Linear_Regression.html#evaluating-model-performance",
    "href": "Extra ML Material/Linear_Regression.html#evaluating-model-performance",
    "title": "Linear Regression: A ‚ÄúHello World‚Äù in Machine Learning",
    "section": "6. Evaluating Model Performance",
    "text": "6. Evaluating Model Performance\nTo know if our model is actually ‚Äúgood,‚Äù we use specific metrics to measure the distance between our predictions (\\(\\hat{y}\\)) and the actual values (\\(y\\)).\n\nMAE (Mean Absolute Error): The average of the absolute differences. It‚Äôs easy to interpret as the ‚Äúaverage error.‚Äù\nRMSE (Root Mean Squared Error): The square root of the average of squared differences. It punishes large outliers more than MAE does.\n\\(R^2\\) (Coefficient of Determination): Represents the proportion of variance explained by the model. \\(1.0\\) is a perfect fit.\n\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Calculate metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse) # RMSE is just the square root of MSE\nr2 = r2_score(y_test, y_pred)\n\nprint(\"--- Model Performance ---\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\nprint(f\"R-squared (R¬≤): {r2:.4f}\")\n\n\n--- Model Performance ---\nMean Absolute Error (MAE): 0.8400\nRoot Mean Squared Error (RMSE): 1.0821\nR-squared (R¬≤): 0.7513"
  },
  {
    "objectID": "Extra ML Material/Linear_Regression.html#what-do-these-numbers-tell-us",
    "href": "Extra ML Material/Linear_Regression.html#what-do-these-numbers-tell-us",
    "title": "Linear Regression: A ‚ÄúHello World‚Äù in Machine Learning",
    "section": "7. What do these numbers tell us?",
    "text": "7. What do these numbers tell us?\nNow that we have our metrics, let‚Äôs break down the ‚Äúvibe‚Äù of our model:\n\nMAE vs RMSE: Notice that RMSE (1.08) is higher than MAE (0.84). This is normal! Because RMSE squares the errors before averaging them, it gives more ‚Äúweight‚Äù to the points that are furthest from the line.\nThe \\(R^2\\) Score: At 0.75, our line is doing a significantly better job than a simple horizontal line representing the mean of \\(y\\)."
  },
  {
    "objectID": "Extra ML Material/Ensembles.html#random-forest-bagging",
    "href": "Extra ML Material/Ensembles.html#random-forest-bagging",
    "title": "Ensemble Learning: Strength in Numbers",
    "section": "1. Random Forest (Bagging)",
    "text": "1. Random Forest (Bagging)\nRandom Forest builds hundreds of trees. Each tree only sees a random ‚Äúbag‚Äù of the data. By averaging them, the ‚Äúextreme‚Äù errors of individual trees cancel each other out.\n\n\nCode\n# Initialize and fit\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\nrf_pred = rf_model.predict(X_test)"
  },
  {
    "objectID": "Extra ML Material/Ensembles.html#gradient-boosting-boosting",
    "href": "Extra ML Material/Ensembles.html#gradient-boosting-boosting",
    "title": "Ensemble Learning: Strength in Numbers",
    "section": "2. Gradient Boosting (Boosting)",
    "text": "2. Gradient Boosting (Boosting)\nUnlike Random Forest, Gradient Boosting builds trees one at a time. Each tree is trained to predict the residuals (the mistakes) of the tree before it.\n\n\nCode\n# Initialize the model\ngb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n\n# Fit the model - using .ravel() to fix the shape of y_train\ngb_model.fit(X_train, y_train.ravel())\n\n# Predict\ngb_pred = gb_model.predict(X_test)"
  },
  {
    "objectID": "Extra ML Material/Ensembles.html#visualizing-the-fit",
    "href": "Extra ML Material/Ensembles.html#visualizing-the-fit",
    "title": "Ensemble Learning: Strength in Numbers",
    "section": "3. Visualizing the ‚ÄúFit‚Äù",
    "text": "3. Visualizing the ‚ÄúFit‚Äù\nNotice how the Ensemble models create a much smoother ‚Äústaircase‚Äù than the single Decision Tree. They follow the data more closely without being as ‚Äútwitchy‚Äù as a single tree.\n\n\nCode\nX_grid = np.arange(0.0, 10.0, 0.01)[:, np.newaxis]\n\nplt.figure(figsize=(12, 6))\nplt.scatter(X, y, s=10, color=\"black\", alpha=0.5, label=\"Data\")\nplt.plot(X_grid, rf_model.predict(X_grid), color=\"green\", label=\"Random Forest (Bagging)\")\nplt.plot(X_grid, gb_model.predict(X_grid), color=\"red\", label=\"Gradient Boosting (Boosting)\")\nplt.legend()\nplt.title(\"Bagging vs. Boosting Comparison\")\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Ensembles.html#final-showdown-metrics",
    "href": "Extra ML Material/Ensembles.html#final-showdown-metrics",
    "title": "Ensemble Learning: Strength in Numbers",
    "section": "4. Final Showdown: Metrics",
    "text": "4. Final Showdown: Metrics\nWhich one performed better? Usually, Boosting is more accurate, but Bagging is harder to overfit.\n\nEnsembles Comparison\n\n\n\n\n\n\n\n\nFeature\nRandom Forest (Bagging)\nGradient Boosting\n\n\n\n\nStrategy\nParallel (Independent trees)\nSequential (Corrective trees)\n\n\nMain Goal\nReduce Overfitting (Variance)\nReduce Underfitting (Bias)\n\n\nKey Tuning\nn_estimators, max_features\nlearning_rate, n_estimators\n\n\n\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport pandas as pd\n\n# Function to calculate all metrics\ndef evaluate_model(y_true, y_pred, name):\n    return {\n        \"Model\": name,\n        \"MAE\": mean_absolute_error(y_true, y_pred),\n        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n        \"R2 Score\": r2_score(y_true, y_pred)\n    }\n\n# Collect results\nresults = [\n    evaluate_model(y_test, rf_pred, \"Random Forest\"),\n    evaluate_model(y_test, gb_pred, \"Gradient Boosting\")\n]\n\n# Display as a clean DataFrame\nresults_df = pd.DataFrame(results)\nprint(results_df.to_string(index=False))\n\n\n            Model      MAE     RMSE  R2 Score\n    Random Forest 0.206577 0.245936  0.898969\nGradient Boosting 0.187138 0.215391  0.922506"
  },
  {
    "objectID": "Extra ML Material/Ensembles.html#analyzing-the-performance-results",
    "href": "Extra ML Material/Ensembles.html#analyzing-the-performance-results",
    "title": "Ensemble Learning: Strength in Numbers",
    "section": "6. Analyzing the Performance Results",
    "text": "6. Analyzing the Performance Results\nOur metrics show a clear ‚Äúwinner‚Äù in terms of raw accuracy, but there is nuance to these numbers:\n\nGradient Boosting (\\(R^2\\): 0.9225): This model is our ‚ÄúSharpshooter.‚Äù By building trees sequentially to fix previous errors, it managed to hug the curve of our synthetic data more tightly than the Random Forest.\nRandom Forest (\\(R^2\\): 0.8989): While slightly less ‚Äúaccurate,‚Äù it is still incredibly strong. Its higher RMSE (0.2459) compared to Boosting (0.2154) suggests it was a bit more affected by the random noise we added to the data."
  },
  {
    "objectID": "Extra ML Material/Time_Series.html#the-setup-synthetic-finance-data",
    "href": "Extra ML Material/Time_Series.html#the-setup-synthetic-finance-data",
    "title": "ARIMA: Time Series Forecasting",
    "section": "1. The Setup & Synthetic Finance Data",
    "text": "1. The Setup & Synthetic Finance Data\nIn finance, stock prices often follow a ‚ÄúRandom Walk‚Äù with a slight upward drift. We will simulate 100 days of stock price movement.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# Set seed for reproducibility\nnp.random.seed(42)\nn_steps = 100\n\n# 1. Create a predictable Trend (upward drift)\ntime = np.arange(n_steps)\ntrend = 0.5 * time\n\n# 2. Add Seasonality (a predictable cycle ARIMA can catch)\nseasonality = 5 * np.sin(2 * np.pi * time / 12)\n\n# 3. Add low-volume Noise\nnoise = np.random.normal(0, 1, n_steps)\n\n# Combine and set starting price\nprices = 100 + trend + seasonality + noise\n\n# Convert to Series\ndates = pd.date_range(start='2023-01-01', periods=n_steps, freq='D')\nts = pd.Series(prices, index=dates)"
  },
  {
    "objectID": "Extra ML Material/Time_Series.html#exploratory-data-analysis-eda",
    "href": "Extra ML Material/Time_Series.html#exploratory-data-analysis-eda",
    "title": "ARIMA: Time Series Forecasting",
    "section": "2. Exploratory Data Analysis (EDA)",
    "text": "2. Exploratory Data Analysis (EDA)\nBefore modeling, we visualize the time series to identify trends, seasonality, and outliers. In finance, we expect to see a ‚Äúnon-stationary‚Äù trend where the price moves away from its starting mean.\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.plot(ts, label=\"Synthetic Stock Price\", color='#2ca02c', linewidth=2)\nplt.title(\"Synthetic Finance Time Series (Daily Close)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price ($)\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Time_Series.html#stationarity-and-arima-parameters-p-d-q",
    "href": "Extra ML Material/Time_Series.html#stationarity-and-arima-parameters-p-d-q",
    "title": "ARIMA: Time Series Forecasting",
    "section": "3. Stationarity and ARIMA Parameters (p, d, q)",
    "text": "3. Stationarity and ARIMA Parameters (p, d, q)\n\nStationarity\nARIMA requires the data to be stationary‚Äîmeaning its mean and variance stay constant over time. Most stock prices are non-stationary because they trend up or down.\nTo fix this, we use the ‚ÄúI‚Äù (Integration) part of ARIMA. By setting d=1, the model looks at the difference between consecutive days (the daily return) rather than the absolute price.\n\n\nSelecting the (p, d, q) Parameters\nWe define our model‚Äôs ‚Äúbrain‚Äù using three numbers: * p (AR - Autoregression): How many past days influence today? We‚Äôll use 5 to capture a business week of momentum. * d (I - Integrated): How many times we difference the data. We‚Äôll use 1 to remove the price trend. * q (MA - Moving Average): How much past forecast errors influence today. We‚Äôll use 0 for this simple example.\nModel Choice: ARIMA(5, 1, 0)\n\n\nCode\n# Split data into Train (80%) and Test (20%)\ntrain_data = ts[:80]\ntest_data = ts[80:]\n\n# Build Model\nmodel = ARIMA(train_data, order=(5, 1, 0))\nmodel_fit = model.fit()\n\n# Summary of the model\nprint(model_fit.summary())\n\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                      y   No. Observations:                   80\nModel:                 ARIMA(5, 1, 0)   Log Likelihood                -155.595\nDate:                Thu, 12 Feb 2026   AIC                            323.189\nTime:                        21:47:29   BIC                            337.406\nSample:                    01-01-2023   HQIC                           328.885\n                         - 03-21-2023                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.1453      0.134      1.082      0.279      -0.118       0.409\nar.L2          0.4018      0.098      4.082      0.000       0.209       0.595\nar.L3          0.1208      0.135      0.894      0.371      -0.144       0.386\nar.L4         -0.4054      0.116     -3.508      0.000      -0.632      -0.179\nar.L5         -0.2903      0.134     -2.170      0.030      -0.552      -0.028\nsigma2         2.9357      0.556      5.283      0.000       1.847       4.025\n===================================================================================\nLjung-Box (L1) (Q):                   2.29   Jarque-Bera (JB):                 0.72\nProb(Q):                              0.13   Prob(JB):                         0.70\nHeteroskedasticity (H):               1.20   Skew:                             0.12\nProb(H) (two-sided):                  0.65   Kurtosis:                         2.59\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
  },
  {
    "objectID": "Extra ML Material/Time_Series.html#forecasting-the-future",
    "href": "Extra ML Material/Time_Series.html#forecasting-the-future",
    "title": "ARIMA: Time Series Forecasting",
    "section": "4. Forecasting the Future",
    "text": "4. Forecasting the Future\nStandard ML models predict a ‚Äútest set‚Äù all at once. ARIMA ‚Äúforecasts‚Äù step-by-step into the future.\nBecause the future is uncertain, the model also provides a Confidence Interval (the shaded area), which represents the range where the price is statistically likely to fall.\n\n\nCode\n# Forecast for the next 20 days\nforecast_obj = model_fit.get_forecast(steps=20)\nforecast_values = forecast_obj.predicted_mean\nconf_int = forecast_obj.conf_int()\n\n# Plotting the results\nplt.figure(figsize=(12, 6))\nplt.plot(train_data, label=\"Training Data\")\nplt.plot(test_data, label=\"Actual Price\", color='gray', alpha=0.5)\nplt.plot(forecast_values, label=\"ARIMA Forecast\", color='red')\nplt.fill_between(conf_int.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\nplt.title(\"Stock Price Forecast vs Actual\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Time_Series.html#model-evaluation",
    "href": "Extra ML Material/Time_Series.html#model-evaluation",
    "title": "ARIMA: Time Series Forecasting",
    "section": "5. Model Evaluation",
    "text": "5. Model Evaluation\nIn Time Series, we use specialized metrics: * AIC (Akaike Information Criterion): Measures the quality of the model relative to others. A lower AIC is better as it rewards accuracy but penalizes over-complexity. * RMSE: The standard deviation of the residuals. In this case, it tells us the average dollar amount the forecast missed by.\n\n\nCode\n# 1. Get AIC from the fitted model\n# (AIC is stored in the model_fit object)\nmodel_aic = model_fit.aic\n\n# 2. Calculate RMSE on the test set\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test_data, forecast_values))\n\nprint(f\"--- ARIMA Model Metrics ---\")\nprint(f\"Akaike Information Criterion (AIC): {model_aic:.2f}\")\nprint(f\"Root Mean Squared Error (RMSE): ${rmse:.4f}\")\n\n\n--- ARIMA Model Metrics ---\nAkaike Information Criterion (AIC): 323.19\nRoot Mean Squared Error (RMSE): $10.0289"
  },
  {
    "objectID": "Extra ML Material/Time_Series.html#interpreting-the-final-results",
    "href": "Extra ML Material/Time_Series.html#interpreting-the-final-results",
    "title": "ARIMA: Time Series Forecasting",
    "section": "6. Interpreting the Final Results",
    "text": "6. Interpreting the Final Results\nOur model achieved an AIC of 323.19 and an RMSE of $10.02. Here is what this tells us about our ‚ÄúPredictable Stock‚Äù:\n\nThe AIC Score: This number is most useful when comparing models. If we tried an ARIMA(1,1,1) and it gave us an AIC of 350, we would know our current (5,1,2) model is statistically superior because it achieves a better fit with less complexity.\nThe RMSE (10.03) This is our ‚Äúuncertainty buffer.‚Äù While the model successfully follows the upward trend and seasonality, any specific daily price prediction is likely to be off by an average of about $10.\n\n\n\nWhy the forecast ‚Äúflattens‚Äù or ‚Äúdrifts‚Äù\nYou may notice that as the forecast goes further into the future, the Confidence Interval (the shaded area) gets wider.\nThis is a key feature of ARIMA‚Äîit acknowledges that uncertainty grows the further we try to look ahead. The model is essentially saying: ‚ÄúI‚Äôm fairly sure about tomorrow, but next week is a much broader guess.‚Äù"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Lab 2026",
    "section": "",
    "text": "This repository serves as the central hub for our Data Science Project Lab. Designed for students to tinker, learn, and observe a full end-to-end data pipeline in action, this is a living project that evolves with our members.\nOur goal is to provide a ‚ÄúGold Standard‚Äù template‚Äîa blueprint that you can use for your own independent research and future professional portfolios.\n\n\n\nOur lab is organized into five core stages that mirror the professional data science lifecycle. Click a stage below to view the technical notebooks:\n\nData Sourcing\nConnecting to datasets via APIs or hosted CSVs to build a reliable foundation.\nPreliminary EDA\nUnderstanding data distributions and performing essential ‚Äúhealth checks‚Äù on our raw information.\nData Wrangling\nThe ‚Äúheavy lifting‚Äù of cleaning, feature engineering, and prepping data for high-performance models.\nAdvanced Visualisation\nInteractive storytelling to turn dry numbers into compelling insights.\nPredictive Modelling\nImplementing everything from baseline statistics to advanced predictive models.\n\n\n\n\n\nTo maintain the quality of our master template, we follow a professional peer-review workflow. If you want to contribute:\n\nStep 1: Access the Code Browse the technical pages using the links above. Each notebook includes a link to open it directly in Google Colab.\nStep 2: Work on a Branch When saving your work back to GitHub, select ‚ÄúCreate new branch‚Äù. Never save directly to main‚Äîthis keeps our production website stable!\nStep 3: Submit a PR Create a Pull Request on GitHub. This allows the Project Lead to review your logic.\nStep 4: Review & Merge Once approved, your changes will be merged into the master template and will appear on this website automatically.\n\n\nReady to start? Use the navigation menu at the top of the page to explore our latest progress in Stage 1 or Stage 2."
  },
  {
    "objectID": "index.html#the-five-stages-of-data-science",
    "href": "index.html#the-five-stages-of-data-science",
    "title": "Project Lab 2026",
    "section": "",
    "text": "Our lab is organized into five core stages that mirror the professional data science lifecycle. Click a stage below to view the technical notebooks:\n\nData Sourcing\nConnecting to datasets via APIs or hosted CSVs to build a reliable foundation.\nPreliminary EDA\nUnderstanding data distributions and performing essential ‚Äúhealth checks‚Äù on our raw information.\nData Wrangling\nThe ‚Äúheavy lifting‚Äù of cleaning, feature engineering, and prepping data for high-performance models.\nAdvanced Visualisation\nInteractive storytelling to turn dry numbers into compelling insights.\nPredictive Modelling\nImplementing everything from baseline statistics to advanced predictive models."
  },
  {
    "objectID": "index.html#contributing-to-the-gold-standard",
    "href": "index.html#contributing-to-the-gold-standard",
    "title": "Project Lab 2026",
    "section": "",
    "text": "To maintain the quality of our master template, we follow a professional peer-review workflow. If you want to contribute:\n\nStep 1: Access the Code Browse the technical pages using the links above. Each notebook includes a link to open it directly in Google Colab.\nStep 2: Work on a Branch When saving your work back to GitHub, select ‚ÄúCreate new branch‚Äù. Never save directly to main‚Äîthis keeps our production website stable!\nStep 3: Submit a PR Create a Pull Request on GitHub. This allows the Project Lead to review your logic.\nStep 4: Review & Merge Once approved, your changes will be merged into the master template and will appear on this website automatically.\n\n\nReady to start? Use the navigation menu at the top of the page to explore our latest progress in Stage 1 or Stage 2."
  },
  {
    "objectID": "1-Data-Sourcing/Data-Sourcing.html#live-api",
    "href": "1-Data-Sourcing/Data-Sourcing.html#live-api",
    "title": "Project Lab: Quick Start Guide",
    "section": "1.2 Live API",
    "text": "1.2 Live API\n\n1.2.1 Get your OpenWeatherMap API Key\nWe are combining static city data with live weather data. You need a personal key to ‚Äútalk‚Äù to the weather server: 1. Go to OpenWeatherMap.org and create a free account. 2. Navigate to your API Keys tab and copy your default key. 3. Note: It can take up to 30-60 minutes for a new key to ‚Äúactivate.‚Äù\n\n\n1.2.2 Set up Colab Secrets\nTo keep our project secure, we never type our API keys directly into the code. * Look at the left-hand sidebar in this Colab window. * Click the Key icon (Secrets) . * Click ‚ÄúAdd new secret‚Äù. * Name: OPENWEATHER_API_KEY * Value: Paste your key here. * Toggle the ‚ÄúNotebook access‚Äù switch to ON.\n\n\nCode\nimport os\nimport pandas as pd\nimport requests\nfrom google.colab import userdata\n\n# --- API SETUP ---\n# Note: This part is more advanced! We use 'userdata' to keep API keys private.\ntry:\n    API_KEY = userdata.get('OPENWEATHER_API_KEY')\n    print(\"API Key found in Colab Secrets.\")\nexcept Exception:\n    API_KEY = None\n    print(\" API Key missing. You can still run this if you have a 'live_weather_cache.csv' file!\")\n\n\nAPI Key found in Colab Secrets.\n\n\n\n\nCode\nimport pandas as pd\nimport requests\n\ndef fetch_live_weather(city_list):\n    \"\"\"\n    Fetches weather data.\n    Priority: 1. GitHub CSV link | 2. Live API calls\n    \"\"\"\n    # Use the RAW URL so Pandas sees the actual CSV text\n    GITHUB_URL = \"https://media.githubusercontent.com/media/LiamDuero03/DS-Society-Project/main/1-Data-Sourcing/live_weather_data.csv\"\n    try:\n        print(\"Checking GitHub for existing weather data...\")\n        # We use a timeout so the script doesn't hang if GitHub is down\n        response = requests.get(GITHUB_URL, timeout=5)\n\n        if response.status_code == 200:\n            print(\"Success! Loading data directly from GitHub.\")\n            # Read the content directly from the response\n            return pd.read_csv(GITHUB_URL)\n        else:\n            print(f\"File not found on GitHub (Status: {response.status_code}).\")\n\n    except Exception as e:\n        print(f\"Could not connect to GitHub: {e}\")\n\n    # --- FALLBACK TO API CALLS ---\n    print(f\"Fetching live data for {len(city_list)} cities via API...\")\n    results = []\n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n\n    for i, city in enumerate(city_list):\n        params = {'q': city, 'appid': API_KEY, 'units': 'metric'}\n        try:\n            res = requests.get(base_url, params=params)\n            if res.status_code == 200:\n                d = res.json()\n                results.append({\n                    'city_name': city.lower().strip(),\n                    'temp': d['main']['temp'],\n                    'condition': d['weather'][0]['description']\n                })\n        except:\n            continue\n\n    return pd.DataFrame(results)\n\n\n\n\nCode\n# --- EXECUTION: Identification & Fetching ---\n\n# 1. Identify our target cities from the raw data we loaded earlier\n# We filter for unique cities and take the top 500 by population\nvalid_unique_cities = (\n    raw_data.dropna(subset=['Population'])\n    .sort_values(by='Population', ascending=False)\n    .drop_duplicates(subset=['City'])\n)\n\ntarget_cities = valid_unique_cities.head(500)['City'].tolist()\n\n# 2. CALL THE FUNCTION\n# This will either load from the CSV cache or call the API\nlive_weather_df = fetch_live_weather(target_cities)\n\n# 3. Quick Preview\nif not live_weather_df.empty:\n    print(\"f Previewing weather for {len(live_weather_df)} cities:\")\n    display(live_weather_df.head())\n\n\nChecking GitHub for existing weather data...\nSuccess! Loading data directly from GitHub.\nf Previewing weather for {len(live_weather_df)} cities:\n\n\n\n    \n\n\n\n\n\n\ncity_name\ntemp\nfeels_like\nhumidity\npressure\ncondition\nwind\n\n\n\n\n0\ntokyo\n4.31\n4.31\n47\n1014\nlight rain\n0.45\n\n\n1\nshanghai\n7.92\n4.27\n66\n1030\nscattered clouds\n7.00\n\n\n2\nbombay\n27.99\n28.53\n51\n1013\nsmoke\n3.60\n\n\n3\nkarachi\n22.90\n21.85\n23\n1017\novercast clouds\n4.12\n\n\n4\nnew delhi\n15.09\n14.67\n77\n1018\nmist\n2.06\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\nCode\n# --- STEP 4: EXPORT YOUR RESULTS ---\n# If you made any changes to the analysis above, run this cell\n# to save your own copy of the data!\n\nif 'live_weather_df' in locals():\n    live_weather_df.to_csv('my_weather_analysis.csv', index=False)\n    print(\" Success! Your results are saved as 'my_weather_analysis.csv'.\")\n    print(\" Click the folder icon on the left to download it to your computer.\")\n\n\n Success! Your results are saved as 'my_weather_analysis.csv'.\n Click the folder icon on the left to download it to your computer."
  },
  {
    "objectID": "3-Data-Wrangling/Data-Wrangling.html#exploratory-data-analysis-eda-identifying-the-issues",
    "href": "3-Data-Wrangling/Data-Wrangling.html#exploratory-data-analysis-eda-identifying-the-issues",
    "title": "3. Data Wrangling & Quality Control",
    "section": "3.1 Exploratory Data Analysis (EDA): Identifying the Issues",
    "text": "3.1 Exploratory Data Analysis (EDA): Identifying the Issues\nBefore we write any cleaning code, we must perform Exploratory Data Analysis. This phase allows us to diagnose the ‚Äúhealth‚Äù of our dataset.\nIn the cell below, we use three primary methods to find our issues: 1. .isnull(): To locate missing data points. 2. .describe(): To find outliers by looking at the minimum and maximum values (checking for impossible temperatures). 3. .unique(): To uncover naming inconsistencies caused by typos or formatting errors.\n\n\nCode\n# Check for Nulls\nprint(\"--- Missing Values ---\")\nprint(df.isnull().sum())\n\n# Check for Outliers (Look at min and max!)\nprint(\"\\n--- Numerical Summary ---\")\nprint(df.describe())\n\n# Check for Naming Inconsistencies\nprint(\"\\n--- Unique Countries ---\")\nprint(df['Country'].unique())\n\n\n--- Missing Values ---\nCountry               0\nTemperature          10\nWeather_Condition    10\ndtype: int64\n\n--- Numerical Summary ---\n       Temperature\ncount    90.000000\nmean     26.552222\nstd     158.285726\nmin    -999.000000\n25%      15.750000\n50%      22.000000\n75%      28.725000\nmax     999.900000\n\n--- Unique Countries ---\n['Mexico' 'USA' 'Canad√¢' 'Canada' 'mexico' ' CANADA ' ' USA ' 'MEXICO '\n 'uSA']"
  },
  {
    "objectID": "3-Data-Wrangling/Data-Wrangling.html#resolving-naming-inconsistencies",
    "href": "3-Data-Wrangling/Data-Wrangling.html#resolving-naming-inconsistencies",
    "title": "3. Data Wrangling & Quality Control",
    "section": "3.2 Resolving Naming Inconsistencies",
    "text": "3.2 Resolving Naming Inconsistencies\nCategorical columns like Country and Weather_Condition often suffer from ‚Äúdata siloing‚Äù where the same value is recorded in different formats.\nTo fix this, we apply a multi-step standardization process: * .str.strip(): Removes accidental leading or trailing spaces. * .str.upper() / .str.capitalize(): Forces uniform casing so ‚Äúusa‚Äù and ‚ÄúUSA‚Äù are treated as the same entity. * .replace(): Manually maps misspellings or special characters (like ‚ÄúCanad√¢‚Äù) to the correct format.\n\n\nCode\n# Clean Country names\ndf['Country'] = df['Country'].str.strip().str.upper()\ndf['Country'] = df['Country'].replace({'CANAD√Ç': 'CANADA', 'MEXICO': 'MEXICO'}) # Standardizing\n\n# Clean Weather Condition\ndf['Weather_Condition'] = df['Weather_Condition'].str.strip().str.capitalize()\ndf['Weather_Condition'] = df['Weather_Condition'].replace({'Snow': 'Snowy'})\n\nprint(\"Fixed Naming Inconsistencies!\")\n\n\nFixed Naming Inconsistencies!"
  },
  {
    "objectID": "3-Data-Wrangling/Data-Wrangling.html#removing-numerical-outliers",
    "href": "3-Data-Wrangling/Data-Wrangling.html#removing-numerical-outliers",
    "title": "3. Data Wrangling & Quality Control",
    "section": "3.3 Removing Numerical Outliers",
    "text": "3.3 Removing Numerical Outliers\nNumerical outliers can severely skew your mean and standard deviation, leading to misleading analysis. In weather data, we can use domain knowledge to identify ‚Äúimpossible‚Äù values.\nSince temperatures like 999¬∞C or -999¬∞C are physically impossible on Earth, we create a logical filter to keep only the data that falls within a realistic range. This ensures our statistical model remains accurate and isn‚Äôt pulled toward extreme sensor errors.\n\n\nCode\n# We define a 'logical' range for weather on Earth\nvalid_range = (df['Temperature'] &gt; -60) & (df['Temperature'] &lt; 60)\ndf = df[valid_range]\n\nprint(f\"Removed outliers. New Max Temp: {df['Temperature'].max()}\")\n\n\nRemoved outliers. New Max Temp: 34.3"
  },
  {
    "objectID": "3-Data-Wrangling/Data-Wrangling.html#handling-missing-values-imputation-removal",
    "href": "3-Data-Wrangling/Data-Wrangling.html#handling-missing-values-imputation-removal",
    "title": "3. Data Wrangling & Quality Control",
    "section": "3.4 Handling Missing Values (Imputation & Removal)",
    "text": "3.4 Handling Missing Values (Imputation & Removal)\nMissing data (NaNs) can be handled in several ways depending on the importance of the column and the type of data it holds:\n\nGrouped Imputation: For Temperature, rather than using a global average, we use a more precise approach by filling missing values with the median temperature of that specific country. This preserves the geographic characteristics of the data.\nTargeted Removal: For Weather_Condition, since we cannot accurately guess if it was ‚ÄúSunny‚Äù or ‚ÄúRainy‚Äù without external data, we choose to drop only the rows where this specific column is null. This ensures we aren‚Äôt introducing false categorical information into our dataset.\n\n\n\nCode\n# --- STEP 1: Fill Temperature based on Country Median ---\n# transform('median') keeps the index the same so it fits back into the dataframe perfectly\ndf['Temperature'] = df['Temperature'].fillna(\n    df.groupby('Country')['Temperature'].transform('median')\n)\n\n# --- STEP 2: Remove rows where Weather_Condition is Null ---\n# We use the 'subset' parameter to ONLY target rows missing weather data\ndf_clean = df.dropna(subset=['Weather_Condition']).copy()\n\nprint(\"1. Temperatures filled using Country-specific medians.\")\nprint(\"2. Rows with missing Weather_Condition have been removed.\")\n\n\n1. Temperatures filled using Country-specific medians.\n2. Rows with missing Weather_Condition have been removed."
  },
  {
    "objectID": "3-Data-Wrangling/Data-Wrangling.html#final-verification",
    "href": "3-Data-Wrangling/Data-Wrangling.html#final-verification",
    "title": "3. Data Wrangling & Quality Control",
    "section": "Final Verification",
    "text": "Final Verification\nThe final step in any data wrangling workflow is verification. We rerun our summary statistics to ensure: * The Temperature range is now realistic (no more 999¬∞ values). * The Unique Values for Country and Weather are standardized and clean. * The Data Quality is consistent across the first few rows of the dataframe.\nBy comparing these results to our initial EDA, we can see exactly how the ‚Äúnoise‚Äù has been filtered out, leaving us with a reliable dataset for analysis.\n\n\nCode\nprint(\"--- Final Cleaned Data Statistics ---\")\nprint(df_clean.describe())\nprint(\"\\n--- Final Unique Values ---\")\nprint(df_clean['Country'].unique())\nprint(df_clean['Weather_Condition'].unique())\n\n# Show the first few rows of the beautiful, clean data\ndf_clean.head()\n\n\n--- Final Cleaned Data Statistics ---\n       Temperature\ncount    78.000000\nmean     21.520513\nstd       7.189375\nmin      10.100000\n25%      15.750000\n50%      20.700000\n75%      28.200000\nmax      34.300000\n\n--- Final Unique Values ---\n['MEXICO' 'USA' 'CANADA']\n['Rainy' 'Cloudy' 'Sunny' 'Snowy']\n\n\n\n    \n\n\n\n\n\n\nCountry\nTemperature\nWeather_Condition\n\n\n\n\n0\nMEXICO\n15.0\nRainy\n\n\n1\nUSA\n10.1\nCloudy\n\n\n2\nMEXICO\n30.4\nRainy\n\n\n4\nCANADA\n28.2\nRainy\n\n\n5\nUSA\n29.3\nSunny"
  },
  {
    "objectID": "Extra ML Material/knn.html#generating-synthetic-clusters",
    "href": "Extra ML Material/knn.html#generating-synthetic-clusters",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "1. Generating Synthetic Clusters",
    "text": "1. Generating Synthetic Clusters\nTo demonstrate K-NN, we are using the make_blobs function to create 300 samples grouped into 3 distinct clusters. This simulates a real-world classification problem, such as grouping customers based on their spending habits and age.\n\nn_samples (300): The total number of data points.\ncenters (3): The number of categories (classes) we want to predict.\ncluster_std (1.5): The ‚Äúspread‚Äù of the data. A higher number makes the clusters overlap, making the classification task more challenging for the algorithm.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\n# 1. Generate clusters with high overlap (increased cluster_std)\nX, y = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n\n# 2. Introduce \"Label Noise\" (flip 10% of the labels randomly)\nrng = np.random.RandomState(42)\nnoise_subset = rng.choice(len(y), size=int(len(y) * 0.10), replace=False)\n\n# Randomly re-assign labels for the noise subset\ny[noise_subset] = rng.randint(0, 3, size=len(noise_subset))"
  },
  {
    "objectID": "Extra ML Material/knn.html#feature-scaling-data-partitioning",
    "href": "Extra ML Material/knn.html#feature-scaling-data-partitioning",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "2. Feature Scaling & Data Partitioning",
    "text": "2. Feature Scaling & Data Partitioning\nSince K-Nearest Neighbors (K-NN) calculates the distance between points (typically Euclidean distance), it is highly sensitive to the scale of the input features. Without scaling, variables with larger magnitudes will dominate the distance calculation.\n\nSteps:\n\nStandardization: Transform features to have a mean of 0 and a standard deviation of 1 using StandardScaler.\nTrain-Test Split: Reserve 20% of the data for testing to ensure the model generalizes well to unseen data.\n\n\n\nCode\n# 2. Scale the data (K-NN is EXTREMELY sensitive to scale because it uses distance)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "Extra ML Material/knn.html#exploratory-data-visualization",
    "href": "Extra ML Material/knn.html#exploratory-data-visualization",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "3. Exploratory Data Visualization",
    "text": "3. Exploratory Data Visualization\nBefore training the model, we visualize the feature space to assess the separability of the classes. Since we are using scaled data, this scatter plot reveals how distinct the clusters are based on the first two features.\n\nKey Observations:\n\nCluster Overlap: Areas where colors mix indicate potential classification errors for K-NN.\nFeature Distribution: Helps verify that the StandardScaler has centered the data appropriately.\nDecision Boundaries: A clearer separation here usually predicts a higher accuracy score.\n\n\n\nCode\n# Visualization of the raw data\nplt.figure(figsize=(8, 6))\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\nplt.title(\"Visualizing the Data Clusters\")\nplt.xlabel(\"Feature 1 (Scaled)\")\nplt.ylabel(\"Feature 2 (Scaled)\")\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/knn.html#model-training-decision-boundary-mapping",
    "href": "Extra ML Material/knn.html#model-training-decision-boundary-mapping",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "4. Model Training & Decision Boundary Mapping",
    "text": "4. Model Training & Decision Boundary Mapping\nWe initialize the K-Nearest Neighbors (K-NN) classifier and prepare the visualization for its decision boundaries. This involves creating a fine ‚Äúmesh grid‚Äù across the feature space and asking the model to predict the class for every single point on that grid.\n\nMethodology:\n\nModel Initialization: Setting \\(k=5\\) to balance the bias-variance tradeoff.\nMesh Grid Generation: We define a coordinate matrix (via meshgrid) that covers the entire range of our scaled features.\nPrediction Surface: By predicting values for the entire grid, we can visualize where the model ‚Äúdecides‚Äù one class ends and another begins.\n\n\nNote: The step size (\\(h = 0.02\\)) determines the resolution of the boundary. A smaller \\(h\\) creates a smoother curve but requires more computational power.\n\n\n\nCode\n# 1. Fit the model with K=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# 2. Create a mesh grid to plot boundaries\nh = .02  # step size in the mesh\nx_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\ny_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# 3. Predict across the entire grid\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)"
  },
  {
    "objectID": "Extra ML Material/knn.html#visualizing-decision-boundaries",
    "href": "Extra ML Material/knn.html#visualizing-decision-boundaries",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "5. Visualizing Decision Boundaries",
    "text": "5. Visualizing Decision Boundaries\nThe final visualization overlays our original data points on top of the model‚Äôs decision regions. This allows us to see exactly how the K-NN algorithm has partitioned the feature space based on the \\(k=5\\) nearest neighbors.\n\nInterpretation Guide:\n\nColored Regions: Represent the ‚Äúpredicted zones.‚Äù Any new data point falling into these areas would be classified accordingly.\nDecision Boundaries: The lines where colors meet. With \\(k=5\\), these boundaries should be relatively smooth; a lower \\(k\\) would make them more jagged (overfitting).\nMisclassifications: Points whose color does not match the background region indicate where the model is struggling or where data overlaps.\n\n\n\nCode\n# 4. Plot the results\nplt.figure(figsize=(10, 8))\nplt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\nplt.title(\"K-NN Decision Boundaries (K=5)\")\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/knn.html#optimization-finding-the-optimal-k-value",
    "href": "Extra ML Material/knn.html#optimization-finding-the-optimal-k-value",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "6. Optimization: Finding the Optimal K-Value",
    "text": "6. Optimization: Finding the Optimal K-Value\nTo move beyond an arbitrary choice of \\(k\\), we implement the Elbow Method. By iterating through a range of \\(k\\) values and calculating the Error Rate, we can identify the point of diminishing returns where the model achieves the best balance between underfitting and overfitting.\n\nThe Logic:\n\nIteration: We train the model 40 times, incrementing \\(k\\) by 1 each time.\nError Tracking: We calculate the mean difference between our predictions (pred_i) and the actual labels (y_test).\nThe ‚ÄúElbow‚Äù: On the resulting plot, we look for the ‚Äúelbow‚Äù‚Äîthe \\(k\\) value where the error rate stabilizes or starts to increase.\n\n\n\nCode\nerror_rate = []\n\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 40), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/knn.html#final-model-evaluation-confusion-matrix",
    "href": "Extra ML Material/knn.html#final-model-evaluation-confusion-matrix",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "7. Final Model Evaluation: Confusion Matrix",
    "text": "7. Final Model Evaluation: Confusion Matrix\nAfter analyzing the Error Rate plot, we select the optimal \\(K\\) (let‚Äôs use \\(K=12\\) given the added noise) to train our final model. We then use a Confusion Matrix to see the specific breakdown of correct vs.¬†incorrect predictions.\n\nWhat to Look For:\n\nDiagonal Elements: High values here indicate correct predictions (True Positives).\nOff-Diagonal Elements: These represent ‚Äúmisclassifications.‚Äù Since we added noise and increased the cluster spread, we expect to see some confusion between overlapping classes.\nPrecision & Recall: A summary of how well the model handles each specific cluster.\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n\n# 1. Choose the best K based on the Elbow plot (e.g., K=12)\nbest_k = 12\nfinal_knn = KNeighborsClassifier(n_neighbors=best_k)\nfinal_knn.fit(X_train, y_train)\nfinal_predictions = final_knn.predict(X_test)\n\n# 2. Generate and Plot the Confusion Matrix\ncm = confusion_matrix(y_test, final_predictions)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Cluster 0', 'Cluster 1', 'Cluster 2'])\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp.plot(ax=ax, cmap='Blues')\nplt.title(f'Confusion Matrix for K={best_k}')\nplt.show()\n\n# 3. Print the Classification Report for detailed metrics\nprint(classification_report(y_test, final_predictions))\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.79      0.60      0.68        25\n           1       0.47      0.62      0.53        13\n           2       0.62      0.68      0.65        22\n\n    accuracy                           0.63        60\n   macro avg       0.63      0.63      0.62        60\nweighted avg       0.66      0.63      0.64        60"
  },
  {
    "objectID": "Extra ML Material/knn.html#performance-analysis-interpretation",
    "href": "Extra ML Material/knn.html#performance-analysis-interpretation",
    "title": "K-Nearest Neighbors (K-NN): The ‚ÄúNeighborhood‚Äù Logic",
    "section": "8. Performance Analysis & Interpretation",
    "text": "8. Performance Analysis & Interpretation\nThe classification report reveals the impact of our high-variance data and label noise. We are seeing a significant drop in performance compared to a ‚Äúclean‚Äù dataset, which is expected in realistic or poorly-behaved data environments.\n\nKey Metrics Breakdown:\n\nPrecision (Cluster 1: 0.47): This is quite low. It means that when the model predicts a point belongs to Cluster 1, it is wrong more than half the time. This cluster is likely heavily ‚Äúinvaded‚Äù by noise from the other two.\nRecall (Cluster 0: 0.60): The model is only catching 60% of the actual members of Cluster 0. Many were likely misclassified into neighboring clusters due to the high standard deviation.\nF1-Score (The Balance): Our highest F1-score is 0.68 (Cluster 0), indicating it is the most ‚Äústable‚Äù cluster, while Cluster 1 is the most problematic (0.53).\n\n\n\nSummary:\nWith an overall accuracy of 63%, the model is performing better than random guessing (which would be 33% for three classes), but the ‚Äúnoise‚Äù we added is successfully preventing the K-NN algorithm from finding clear, reliable boundaries."
  },
  {
    "objectID": "Extra ML Material/Deep_Learning.html#generating-high-complexity-synthetic-data",
    "href": "Extra ML Material/Deep_Learning.html#generating-high-complexity-synthetic-data",
    "title": "Neural Networks: Mimicking Biological Logic",
    "section": "1. Generating High-Complexity Synthetic Data",
    "text": "1. Generating High-Complexity Synthetic Data\nNeural Networks are ‚Äúdata hungry.‚Äù To make a Neural Network worth using, we need a problem that is too complex for simple linear models to solve.\nWe will generate 2,000 samples with three features (\\(x_1, x_2, x_3\\)) and a target (\\(y\\)) that follows a non-linear ‚Äúsaddle‚Äù function: \\[y = x_1^2 - x_2^2 + \\sin(x_3) + \\text{noise}\\]\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Generate 2,000 samples (NNs need more data)\nnp.random.seed(42)\nn_samples = 2000\nX = np.random.uniform(-5, 5, (n_samples, 3))\n\n# Target: A complex interaction of the features\n# y = x1^2 - x2^2 + sin(x3) + noise\ny = X[:, 0]**2 - X[:, 1]**2 + np.sin(X[:, 2]) + np.random.normal(0, 0.5, n_samples)"
  },
  {
    "objectID": "Extra ML Material/Deep_Learning.html#feature-scaling-data-splitting",
    "href": "Extra ML Material/Deep_Learning.html#feature-scaling-data-splitting",
    "title": "Neural Networks: Mimicking Biological Logic",
    "section": "2. Feature Scaling & Data Splitting",
    "text": "2. Feature Scaling & Data Splitting\n\nThe Importance of Scaling\nNeural Networks learn by adjusting ‚Äúweights‚Äù through Gradient Descent. If your features have different ranges, the gradients can become unstable, causing the model to take forever to learn or even fail entirely.\nWe use StandardScaler to transform our data so that: * Mean = 0 * Standard Deviation = 1\nThis creates a ‚Äúlevel playing field‚Äù for all input features.\n\n\nCode\n# Neural Networks are sensitive to scale!\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)"
  },
  {
    "objectID": "Extra ML Material/Deep_Learning.html#building-the-neural-network-the-brain",
    "href": "Extra ML Material/Deep_Learning.html#building-the-neural-network-the-brain",
    "title": "Neural Networks: Mimicking Biological Logic",
    "section": "3. Building the Neural Network (The ‚ÄúBrain‚Äù)",
    "text": "3. Building the Neural Network (The ‚ÄúBrain‚Äù)\nWe are using a Sequential model, which means our data flows in one direction through a series of layers.\n\nThe Layers:\n\nInput Layer: Designed to receive our 3 features (\\(x_1, x_2, x_3\\)).\nHidden Layer 1 (64 Neurons): Uses the ReLU activation function. This is the ‚Äúmagic‚Äù that allows the network to learn non-linear patterns like our saddle shape.\nHidden Layer 2 (32 Neurons): A second layer to refine the patterns identified in the first layer.\nOutput Layer (1 Neuron): Since this is a Regression task (predicting a number), the final layer has a single neuron with no activation function.\n\n\n\nActivation Functions: ReLU\nWe use ReLU (Rectified Linear Unit) because it is computationally efficient and helps prevent the ‚Äúvanishing gradient‚Äù problem, allowing deeper networks to learn faster.\n\n\nCode\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(3,)),\n    Dense(32, activation='relu'),\n    Dense(1) # Final output\n])\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\nmodel.summary()\n\n\n/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 64)             ‚îÇ           256 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 32)             ‚îÇ         2,080 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 1)              ‚îÇ            33 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 2,369 (9.25 KB)\n\n\n\n Trainable params: 2,369 (9.25 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "Extra ML Material/Deep_Learning.html#training-the-learning-process",
    "href": "Extra ML Material/Deep_Learning.html#training-the-learning-process",
    "title": "Neural Networks: Mimicking Biological Logic",
    "section": "5. Training: The Learning Process",
    "text": "5. Training: The Learning Process\nNow we ‚Äúfit‚Äù the model to our data. Unlike simpler models that solve in one go, Neural Networks learn iteratively over several passes.\n\nEpochs (50): The number of times the model will see the entire training dataset.\nBatch Size (32): The model doesn‚Äôt look at all 1,600 training points at once; it looks at 32 at a time, updates its weights, and moves on. This makes training faster and more stable.\nValidation Split (0.2): We hold back 20% of the training data as a ‚Äúmini-test‚Äù during each epoch. This helps us see if the model is learning general patterns or just memorizing the training data (Overfitting).\n\n\n\nCode\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=50,\n    batch_size=32,\n    verbose=0 # Keep the notebook clean\n)\n\n# Plotting the Learning Curve\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Convergence')\nplt.xlabel('Epochs')\nplt.ylabel('MSE Loss')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Deep_Learning.html#evaluating-the-neural-network",
    "href": "Extra ML Material/Deep_Learning.html#evaluating-the-neural-network",
    "title": "Neural Networks: Mimicking Biological Logic",
    "section": "7. Evaluating the Neural Network",
    "text": "7. Evaluating the Neural Network\nNow that the training is complete, we put our model to the test. We use the Test Set (data the model has never seen) to calculate our final performance metrics.\n\nR-squared (\\(R^2\\)): Tells us how much of the complex ‚Äúsaddle‚Äù shape variance the model captured. For this non-linear data, a high \\(R^2\\) demonstrates that the hidden layers successfully learned the mathematical interactions between \\(x_1, x_2\\), and \\(x_3\\).\nMAE (Mean Absolute Error): Represents the average physical distance between our model‚Äôs predictions and the actual values.\n\n\n\nCode\nfrom sklearn.metrics import r2_score\n\ny_pred = model.predict(X_test)\nnn_r2 = r2_score(y_test, y_pred)\nnn_mae = np.mean(np.abs(y_test - y_pred.flatten()))\n\nprint(f\"--- Neural Network Performance ---\")\nprint(f\"R-squared (R¬≤): {nn_r2:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {nn_mae:.4f}\")\n\n\n\n13/13 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step \n\n--- Neural Network Performance ---\n\nR-squared (R¬≤): 0.9901\n\nMean Absolute Error (MAE): 0.8774"
  },
  {
    "objectID": "Extra ML Material/Deep_Learning.html#analyzing-the-results-the-power-of-deep-learning",
    "href": "Extra ML Material/Deep_Learning.html#analyzing-the-results-the-power-of-deep-learning",
    "title": "Neural Networks: Mimicking Biological Logic",
    "section": "8. Analyzing the Results: The Power of Deep Learning",
    "text": "8. Analyzing the Results: The Power of Deep Learning\nAn \\(R^2\\) of 0.99 indicates that the model has captured 99% of the variance in our complex dataset. This is a significant jump compared to what a linear model would achieve on the same data.\n\nHigh Precision: An MAE of 0.87 relative to the range of our target variable shows that the ‚Äúneurons‚Äù have successfully approximated the non-linear saddle function.\nGeneralization: Since our Validation Loss and Test Metrics are both strong, we know the model didn‚Äôt just ‚Äúmemorize‚Äù the training points (overfitting)‚Äîit actually learned the underlying physics of the data.\n\n\nFinal Verdict: Neural Networks\nWhile the performance is the highest we‚Äôve seen, it‚Äôs important to remember the ‚Äúcost‚Äù of this accuracy:\n\n\n\n\n\n\n\nFactor\nObservation\n\n\n\n\nCompute Power\nThis model required 50 iterations (epochs) to reach this result.\n\n\nData Scaling\nWithout the StandardScaler, this model would likely have failed.\n\n\nInterpretability\nWe know the model is accurate, but we can‚Äôt ‚Äúread‚Äù the weights like a simple slope in Linear Regression."
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#the-setup",
    "href": "Extra ML Material/Decision_Trees.html#the-setup",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "1. The Setup",
    "text": "1. The Setup\nWe‚Äôll use DecisionTreeRegressor for the model and plot_tree to visualize the actual logic the computer builds.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#generate-non-linear-synthetic-data",
    "href": "Extra ML Material/Decision_Trees.html#generate-non-linear-synthetic-data",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "2. Generate Non-Linear Synthetic Data",
    "text": "2. Generate Non-Linear Synthetic Data\nDecision Trees excel at capturing curves. We will generate a sine wave with some added noise to simulate a real-world scenario where a simple straight line would fail.\n\n\nCode\n# Generate non-linear synthetic data\nnp.random.seed(42)\nX = np.sort(5 * np.random.rand(80, 1), axis=0)\ny = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#training-the-model",
    "href": "Extra ML Material/Decision_Trees.html#training-the-model",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "3. Training the Model",
    "text": "3. Training the Model\nIn Decision Trees, the most important setting is max_depth. * Too shallow: The model is too simple (Underfitting). * Too deep: The model memorizes the noise (Overfitting).\nWe will start with a depth of 3 to keep the logic interpretable.\n\n\nCode\n# Initialize and fit the model\n# Let's start with a depth of 3\ntree_reg = DecisionTreeRegressor(max_depth=3)\ntree_reg.fit(X_train, y_train)\n\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=3)"
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#visualizing-the-decision-logic",
    "href": "Extra ML Material/Decision_Trees.html#visualizing-the-decision-logic",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "4. Visualizing the Decision Logic",
    "text": "4. Visualizing the Decision Logic\nThe ‚ÄúTree‚Äù starts at the Root Node and splits until it reaches the Leaf Nodes. Each split is chosen to minimize the error (MSE) within the resulting groups.\n\n\nCode\nplt.figure(figsize=(15, 8))\nplot_tree(tree_reg, filled=True, feature_names=['X'], precision=2)\nplt.title(\"Decision Tree Logic Structure\")\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#visualizing-the-prediction-line",
    "href": "Extra ML Material/Decision_Trees.html#visualizing-the-prediction-line",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "5. Visualizing the Prediction Line",
    "text": "5. Visualizing the Prediction Line\nNotice that the prediction line isn‚Äôt smooth‚Äîit looks like a series of steps. This is because every data point within a specific ‚Äúleaf‚Äù gets the same predicted value (the average of the training points in that leaf).\n\n\nCode\n# Predict\nX_grid = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_grid_pred = tree_reg.predict(X_grid)\n\nplt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\nplt.plot(X_grid, y_grid_pred, color=\"cornflowerblue\", label=\"prediction\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression (Step Function)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#model-evaluation",
    "href": "Extra ML Material/Decision_Trees.html#model-evaluation",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "6. Model Evaluation",
    "text": "6. Model Evaluation\nWe use the same metrics as Linear Regression to see how well our tree is performing.\n\n\nCode\ny_pred = tree_reg.predict(X_test)\n\nprint(\"--- Decision Tree Performance ---\")\nprint(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\nprint(f\"R¬≤ Score: {r2_score(y_test, y_pred):.4f}\")\n\n\n--- Decision Tree Performance ---\nMAE: 0.1366\nRMSE: 0.1599\nR¬≤ Score: 0.9381"
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#analyzing-the-results",
    "href": "Extra ML Material/Decision_Trees.html#analyzing-the-results",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "7. Analyzing the Results",
    "text": "7. Analyzing the Results\nWith an \\(R^2\\) Score of 0.9381, this model is performing exceptionally well.\nCompared to our Linear Regression model, the errors (MAE and RMSE) are much lower. This is because the Decision Tree can ‚Äúbend‚Äù its predictions to follow the curves of the sine wave, whereas a linear model would be forced to stay in a straight line."
  },
  {
    "objectID": "Extra ML Material/Decision_Trees.html#the-danger-zone-overfitting",
    "href": "Extra ML Material/Decision_Trees.html#the-danger-zone-overfitting",
    "title": "Decision Tree Regression: Logic-Based Prediction",
    "section": "8. The Danger Zone: Overfitting",
    "text": "8. The Danger Zone: Overfitting\nWhile a score of 0.93 is great, Decision Trees have a tendency to overfit.\n\nIf we set max_depth to 20, the \\(R^2\\) might go to 0.99, but the model would just be ‚Äúconnecting the dots‚Äù of the noise.\nIf we set max_depth to 1, we would get a very high error because the model is too simple.\n\n\nSummary of Decision Tree Regressors\n\n\n\nPros\nCons\n\n\n\n\nCaptures non-linear relationships\nVery prone to overfitting\n\n\nEasy to visualize and explain\nSensitive to small changes in data\n\n\nNo need for feature scaling\nCan become overly complex (deep)"
  },
  {
    "objectID": "4-Data-preprocessing/Data-Preprocessing.html",
    "href": "4-Data-preprocessing/Data-Preprocessing.html",
    "title": "4. Data Preprocessing & Feature Engineering: Urban Weather Analysis",
    "section": "",
    "text": "4. Data Preprocessing & Feature Engineering: Urban Weather Analysis\nThis session builds directly upon the data initially pulled: the Location dataset (containing city coordinates and population) and the Weather dataset (containing atmospheric conditions).\nOur objective is to transform these raw inputs into a refined, model-ready dataset designed to predict the feels_like temperature. Throughout this notebook, we will demonstrate professional data handling techniques including:\n\nRelational Merging: Unifying geographic and atmospheric data points.\nLeak-Proof Feature Engineering: Creating a ‚ÄúWind-Moisture Interaction‚Äù index while strictly avoiding data leakage from our target variable.\nCategorical Encoding: Utilizing One-Hot Encoding to transform qualitative weather conditions into machine-readable formats.\nFeature Scaling: Applying Standardization (\\(Z\\)-score scaling) to the population data to handle variance and the Urban Heat Island effect.\n\n\n\nCode\n# --- LFS SETUP & REPO CLONING ---\nimport os\nimport pandas as pd\n\n# 1. Install Git LFS in the Colab environment\n!git lfs install\n\n# 2. Clone the repository (This pulls the LFS pointers)\nREPO_NAME = \"DS-Society-Project\"\nREPO_URL = f\"https://github.com/LiamDuero03/{REPO_NAME}.git\"\n\nif not os.path.exists(REPO_NAME):\n    !git clone {REPO_URL}\nelse:\n    print(\"Repository already cloned.\")\n\n# 3. Explicitly pull the LFS data (This replaces pointers with actual CSV data)\n%cd {REPO_NAME}\n!git lfs pull\n%cd ..\n\n# --- 4. READ THE DATA ---\n# Now we point to the LOCAL folder inside Colab, not the web URL\ncities_path = f\"/content/{REPO_NAME}/1-Data-Sourcing/all_cities_raw.csv\"\nweather_path = f\"/content/{REPO_NAME}/1-Data-Sourcing/live_weather_data.csv\"\n\nraw_data = pd.read_csv(cities_path, low_memory=False)\nlive_weather_df = pd.read_csv(weather_path)\n\nprint(f\"Success! Cities Shape: {raw_data.shape}\")\nprint(f\"Success! Weather Shape: {live_weather_df.shape}\")\n\n\nGit LFS initialized.\nCloning into 'DS-Society-Project'...\nremote: Enumerating objects: 177, done.\nremote: Counting objects: 100% (177/177), done.\nremote: Compressing objects: 100% (134/134), done.\nremote: Total 177 (delta 72), reused 77 (delta 24), pack-reused 0 (from 0)\nReceiving objects: 100% (177/177), 2.40 MiB | 6.80 MiB/s, done.\nResolving deltas: 100% (72/72), done.\n/content/DS-Society-Project\n/content\nSuccess! Cities Shape: (5000, 4)\nSuccess! Weather Shape: (492, 7)\n\n\n\n4.1 Merging the Datasets\nSince your columns are named City in the first DataFrame and city_name in the second, we need to specify that mapping. A left join is usually safest here to keep all your primary geographic data.\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Assuming df_geo and df_weather are your dataframes\ndf = pd.merge(raw_data, live_weather_df, left_on='City', right_on='city_name', how='left')\n\n# Drop the redundant city_name column\ndf.drop('city_name', axis=1, inplace=True)\n\n\n\n\n4.2 Categorical Encoding\nThe condition column (e.g., Sunny, Rainy) is categorical. Since there isn‚Äôt necessarily a mathematical order to weather conditions, One-Hot Encoding is the standard approach to transform these strings into a format the model can interpret without assuming a ranking.\n\n\nCode\n# One-Hot Encoding the 'condition' column\n# This creates new binary columns for each unique weather condition\ndf = pd.get_dummies(df, columns=['condition'], prefix='weather')\n\n\n\n\n4.3 Feature Engineering: Interaction Features\nTo improve the model‚Äôs ability to predict feels_like, we create an interaction feature. High humidity combined with low wind speed often results in a ‚Äústuffy‚Äù or higher perceived temperature. This provides a non-linear signal that helps the model learn patterns beyond simple linear correlations.\n\n\nCode\n# Interaction feature: High humidity + Low wind often feels \"stuffy\"\n# This provides a non-linear signal that helps the model predict 'feels_like'\ndf['moisture_wind_ratio'] = df['humidity'] / (df['wind'] + 1) # +1 to avoid division by zero\n\n\n\n\n4.4 Standardization vs.¬†Normalization\nFor variables like Population, which often have large outliers or a wide range, Standardization (Z-score scaling) is usually preferred. This process centers the data around a mean of 0 with a standard deviation of 1, ensuring that the model isn‚Äôt disproportionately biased by the high magnitude of population figures.\n\n\nCode\nscaler = StandardScaler()\n\n# Standardizing the 'Population' column\n# Formula: z = (x - u) / s\ndf['Population_scaled'] = scaler.fit_transform(df[['Population']])\n\n\n\n\n4.5 Final Data Transformation Review\nWith the preprocessing pipeline complete, we verify the results. This step ensures that the merge was successful, categorical variables are correctly expanded, and our engineered features and scaled columns are present.\n\n\nCode\n## --- 1. PREVIEW THE FINAL DATAFRAME ---\nprint(\"Final Dataframe Shape:\", df.shape)\nprint(\"\\nFirst 5 rows of processed data:\")\ndisplay(df.head())\n\n\nFinal Dataframe Shape: (5000, 29)\n\nFirst 5 rows of processed data:\n\n\n\n    \n\n\n\n\n\n\nCity\nPopulation\nLatitude\nLongitude\ntemp\nfeels_like\nhumidity\npressure\nwind\nweather_broken clouds\n...\nweather_moderate rain\nweather_overcast clouds\nweather_sand\nweather_scattered clouds\nweather_smoke\nweather_snow\nweather_thunderstorm\nweather_thunderstorm with light rain\nmoisture_wind_ratio\nPopulation_scaled\n\n\n\n\n0\ntokyo\n31480498.0\n35.685000\n139.751389\n4.31\n4.31\n47.0\n1014.0\n0.45\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n32.413793\n35.093621\n\n\n1\nshanghai\n14608512.0\n31.045556\n121.399722\n7.92\n4.27\n66.0\n1030.0\n7.00\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n8.250000\n16.084495\n\n\n2\nbombay\n12692717.0\n18.975000\n72.825833\n27.99\n28.53\n51.0\n1013.0\n3.60\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n11.086957\n13.926030\n\n\n3\nkarachi\n11627378.0\n24.905600\n67.082200\n22.90\n21.85\n23.0\n1017.0\n4.12\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n4.492188\n12.725747\n\n\n4\nnew delhi\n10928270.0\n28.600000\n77.200000\n15.09\n14.67\n77.0\n1018.0\n2.06\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n25.163399\n11.938084\n\n\n\n\n5 rows √ó 29 columns"
  }
]