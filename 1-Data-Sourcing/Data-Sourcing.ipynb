{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ffea47",
   "metadata": {},
   "source": [
    "# üöÄ Project Lab: Quick Start Guide\n",
    "\n",
    "Welcome to our first society project!\n",
    "\n",
    "Our problem statement: Can we predict a city's 'Feels Like' temperature (Apparent Temperature) based solely on its size, location, and humidity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a502b861",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d132b05",
   "metadata": {},
   "source": [
    "# 1. Data Sourcing & Integration\n",
    "In this section, we demonstrate how a Data Scientist pulls data from two distinct sources:\n",
    "1. **Local Data Sourcing:** Internal \"proprietary\" data (e.g., City Population).\n",
    "2. **Live API:** Real-time data enrichment (Live Weather)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62df35",
   "metadata": {},
   "source": [
    "### 1.1 Local Data Sourcing (Automated)\n",
    "Instead of manual uploads, we pull our \"internal\" population data directly from our GitHub repository. This ensures all members are working with the same version of the data.\n",
    "\n",
    "* **Primary Data Source:** The underlying dataset is the **World Cities Database**, originally sourced from [Kaggle](https://www.kaggle.com/datasets/max-mind/world-cities-database).\n",
    "* **Storage Method:** Due to the large file size (3.1+ million rows), the CSV is hosted using **Git LFS (Large File Storage)**. This allows us to bypass GitHub's standard file limits and stream the data directly into our environment via Raw GitHub URLs.\n",
    "* **Automation:** By using `pd.read_csv()` on the hosted URL, we eliminate the need for users to download local copies or manage large files manually.\n",
    "\n",
    "| Column | Description |\n",
    "| :--- | :--- |\n",
    "| **city** | Standardized city name (lowercased for merging) |\n",
    "| **pop** | Total population count |\n",
    "| **lat / lng** | Geographic coordinates (Latitude and Longitude) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116746a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "GITHUB_CSV_URL = \"https://media.githubusercontent.com/media/LiamDuero03/DS-Society-Project/refs/heads/main/worldcitiespop.csv\"\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "try:\n",
    "    # Use 'usecols' to load only what we need (saves RAM)\n",
    "    # Use lowercase column names if your CSV has them; otherwise keep these\n",
    "    internal_metadata_df = pd.read_csv(\n",
    "        GITHUB_CSV_URL,\n",
    "        low_memory=False,\n",
    "        usecols=['City', 'Population', 'Latitude', 'Longitude']\n",
    "    )\n",
    "\n",
    "    # Standardize column names for ease of use\n",
    "    internal_metadata_df.columns = ['city', 'pop', 'lat', 'lng']\n",
    "\n",
    "    print(f\"‚úÖ Successfully loaded {len(internal_metadata_df):,} rows from GitHub.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Tip: Check your GitHub LFS settings or the file URL.\")\n",
    "\n",
    "# --- 2. DATA CLEANING & SELECTION ---\n",
    "# We focus on unique cities and prioritize the records with the highest population\n",
    "valid_unique_cities = (\n",
    "    internal_metadata_df\n",
    "    .dropna(subset=['pop'])               # Remove rows without population data\n",
    "    .sort_values(by='pop', ascending=False) # Put largest cities at the top\n",
    "    .drop_duplicates(subset=['city'])     # Keep only the largest version of each city\n",
    ")\n",
    "\n",
    "# Extract the top 50 cities for our API target list\n",
    "target_cities = valid_unique_cities.head(50)['city'].tolist()\n",
    "\n",
    "# --- 3. SUMMARY OUTPUT ---\n",
    "print(f\"‚úÖ Selected the top {len(target_cities)} unique megacities for analysis.\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Top 5 Cities: {', '.join(target_cities[:5])}...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Display the first few rows of our cleaned dataset\n",
    "valid_unique_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646ce6d",
   "metadata": {},
   "source": [
    "## 1.2 Live API\n",
    "#### 1.2.1 Get your OpenWeatherMap API Key\n",
    "\n",
    "We are combining static city data with live weather data. You need a personal key to \"talk\" to the weather server:\n",
    "1. Go to [OpenWeatherMap.org](https://openweathermap.org/api) and create a free account.\n",
    "2. Navigate to your **API Keys** tab and copy your default key.\n",
    "3. *Note:* It can take up to 30-60 minutes for a new key to \"activate.\"\n",
    "\n",
    "#### 1.2.2 üõ°Ô∏è Set up Colab Secrets\n",
    "To keep our project secure, we **never** type our API keys directly into the code.\n",
    "* Look at the left-hand sidebar in this Colab window.\n",
    "* Click the **Key icon (Secrets)** üîë.\n",
    "* Click \"Add new secret\".\n",
    "* Name: `OPENWEATHER_API_KEY`\n",
    "* Value: Paste your key here.\n",
    "* **Toggle the \"Notebook access\" switch to ON.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List of files to remove\n",
    "files_to_delete = ['live_weather_cache.csv']\n",
    "\n",
    "for file in files_to_delete:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"üóëÔ∏è Deleted: {file}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {file} not found (already deleted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaae002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from google.colab import userdata\n",
    "\n",
    "# --- 1. API SETUP & CACHING LOGIC ---\n",
    "try:\n",
    "    API_KEY = userdata.get('OPENWEATHER_API_KEY')\n",
    "    print(\"‚úÖ OpenWeather API Key successfully retrieved.\")\n",
    "except:\n",
    "    print(\"‚ùå API Key missing. Please add 'OPENWEATHER_API_KEY' to Colab Secrets.\")\n",
    "\n",
    "def fetch_live_weather(city_list):\n",
    "    \"\"\"\n",
    "    Fetches real-time weather data for a list of cities.\n",
    "    Saves results to a local CSV to prevent unnecessary API calls.\n",
    "    \"\"\"\n",
    "    CACHE_FILE = \"live_weather_cache.csv\"\n",
    "\n",
    "    # Check if we already have data from a previous run\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        print(f\"üì¶ Loading weather data from local cache: {CACHE_FILE}\")\n",
    "        return pd.read_csv(CACHE_FILE)\n",
    "\n",
    "    print(f\"üåê Cache not found. Fetching live data for {len(city_list)} cities...\")\n",
    "    results = []\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    # Loop through cities and collect data\n",
    "    for i, city in enumerate(city_list):\n",
    "        params = {'q': city, 'appid': API_KEY, 'units': 'metric'}\n",
    "\n",
    "        # Simple progress indicator for beginners\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"   Progress: {i + 1}/{len(city_list)} cities processed...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                d = response.json()\n",
    "                results.append({\n",
    "                    'city_name': city.lower().strip(),\n",
    "                    'temp': d['main']['temp'],\n",
    "                    'feels_like': d['main']['feels_like'],\n",
    "                    'humidity': d['main']['humidity'],\n",
    "                    'pressure': d['main']['pressure'],\n",
    "                    'condition': d['weather'][0]['description'],\n",
    "                    'wind': d['wind']['speed']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching {city}: {e}\")\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    weather_df = pd.DataFrame(results)\n",
    "    weather_df.to_csv(CACHE_FILE, index=False)\n",
    "    print(\"‚úÖ API calls complete. Results cached locally.\")\n",
    "    return weather_df\n",
    "\n",
    "# --- 2. TARGET SELECTION ---\n",
    "# We prioritize the top 50 unique cities with the highest verified populations\n",
    "print(\"üßπ Cleaning population data to identify the top 50 target cities...\")\n",
    "\n",
    "# This logic ensures we get unique names and the highest recorded pop for each\n",
    "valid_unique_cities = (\n",
    "    internal_metadata_df.dropna(subset=['pop'])\n",
    "    .sort_values(by='pop', ascending=False)\n",
    "    .drop_duplicates(subset=['city'])\n",
    ")\n",
    "\n",
    "target_cities = valid_unique_cities.head(50)['city'].tolist()\n",
    "\n",
    "# --- 3. EXECUTION ---\n",
    "live_weather_df = fetch_live_weather(target_cities)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"üìä Weather Data Preview ({len(live_weather_df)} cities):\")\n",
    "live_weather_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
